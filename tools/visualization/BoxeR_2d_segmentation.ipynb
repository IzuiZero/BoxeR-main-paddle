{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from e2edet.utils import visualization as vis\n",
    "from e2edet.dataset.helper import CocoDetection\n",
    "from e2edet.dataset.coco import convert_coco_poly_to_mask\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get(\"E2E_DATASETS\") is None:\n",
    "    warnings.warn(\"E2E_DATASETS environment should be imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catergories_path = os.path.join(os.environ[\"E2E_DATASETS\"], \"coco/vocabs/coco_categories.txt\")\n",
    "categories = []\n",
    "with open(catergories_path) as f:\n",
    "    categories = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annFile = os.path.join(os.environ[\"E2E_DATASETS\"], \"coco/annotation/instances_val2017.json\")\n",
    "image_path = os.path.join(os.environ[\"E2E_DATASETS\"], \"coco/image/val2017/{:012d}.jpg\")\n",
    "val_coco = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = CocoDetection(os.path.join(os.environ[\"E2E_DATASETS\"], \"coco/image/val2017\"), annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_visualize_gt(image, target, show_mask=False):\n",
    "    img_w, img_h = image.size\n",
    "    \n",
    "    gts = []\n",
    "    for item in target:\n",
    "        x, y, w, h = item[\"bbox\"]\n",
    "        \n",
    "        processed_item = {\"bbox\": [x, y, x+w, y+h], \"category_id\": item[\"category_id\"]}\n",
    "        if show_mask:\n",
    "            processed_item[\"mask\"] = convert_coco_poly_to_mask([item[\"segmentation\"]], img_h, img_w)[0]\n",
    "        gts.append(processed_item)\n",
    "        \n",
    "    img = np.array(image)[:,:,::-1].copy()\n",
    "    for i, gt in enumerate(gts):\n",
    "        color = vis.get_color(i)\n",
    "        \n",
    "        bbox2d = torch.tensor(gt[\"bbox\"])\n",
    "        position = np.array(gt[\"bbox\"][:2])\n",
    "        label = categories[gt[\"category_id\"]]\n",
    "        vis.draw_bbox2d(img, bbox2d, color=color, thickness=2)\n",
    "        vis.draw_text(img, label, position, color=(0,0,0))\n",
    "        if show_mask:\n",
    "            vis.apply_mask(img, gt[\"mask\"], color, alpha=0.5)\n",
    "        \n",
    "    print(img.shape)\n",
    "    vis.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = valset[255]\n",
    "load_and_visualize_gt(image, target, show_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2edet.utils.configuration import load_yaml\n",
    "from e2edet.utils.general import get_root\n",
    "from e2edet.dataset.processor import build_processor\n",
    "from e2edet.dataset.coco import ConvertCocoPolysToMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_visualize(image, target, show_mask=False):    \n",
    "    gts = []\n",
    "    for i in range(target[\"boxes\"].shape[0]):\n",
    "        x1, y1, x2, y2 = target[\"boxes\"][i].tolist()\n",
    "        \n",
    "        processed_item = {\"bbox\": [x1, y1, x2, y2], \"category_id\": target[\"labels\"][i]}\n",
    "        if show_mask:\n",
    "            processed_item[\"mask\"] = target[\"masks\"][i]\n",
    "        gts.append(processed_item)\n",
    "        \n",
    "    img = np.array(image)[:,:,::-1].copy()\n",
    "    for i, gt in enumerate(gts):\n",
    "        color = vis.get_color(i)\n",
    "        \n",
    "        bbox2d = torch.tensor(gt[\"bbox\"])\n",
    "        position = np.array(gt[\"bbox\"][:2])\n",
    "        label = categories[gt[\"category_id\"]]\n",
    "        vis.draw_bbox2d(img, bbox2d, color=color, thickness=2)\n",
    "        vis.draw_text(img, label, position, color=(0,0,0))\n",
    "        if show_mask:\n",
    "            vis.apply_mask(img, gt[\"mask\"], color, alpha=0.5)\n",
    "        \n",
    "    print(img.shape)\n",
    "    vis.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(get_root(), \"config/COCO-InstanceSegmentation\")\n",
    "config_path = os.path.join(log_dir, \"boxer2d_R_101_3x.yaml\")\n",
    "\n",
    "config = load_yaml(config_path)\n",
    "image_processor_config = config.dataset_config.detection.processors.image_train_processor\n",
    "image_processor = build_processor(image_processor_config)\n",
    "prepare = ConvertCocoPolysToMask(config.dataset_config.detection[\"use_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 265\n",
    "image, target = valset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_visualize_gt(image, target, show_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "from e2edet.utils.configuration import load_yaml\n",
    "from e2edet.utils.general import get_root\n",
    "from e2edet.dataset.processor import build_processor\n",
    "from e2edet.dataset import build_dataset\n",
    "from e2edet.model import build_model\n",
    "from e2edet.dataset.coco import ConvertCocoPolysToMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(get_root(), \"..\", \"save/COCO-InstanceSegmentation\")\n",
    "model_path = \"boxer2d_R_101_3x/boxer2d_final.pth\"\n",
    "config_path = \"boxer2d_R_101_3x/config.yaml\"\n",
    "vis_path = \"boxer2d_R_101_3x/vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoxeRDemo:\n",
    "    def __init__(self, root_path, model_path, config_path, current_device=torch.device(\"cpu\")):\n",
    "        model_path = os.path.join(root_path, model_path)\n",
    "        config_path = os.path.join(root_path, config_path)\n",
    "        self.current_device = current_device\n",
    "        print(\"Loading model from\", model_path)\n",
    "        \n",
    "        self.config = load_yaml(config_path)\n",
    "        self._init_processors()\n",
    "        self.model = self._build_boxer(model_path)\n",
    "\n",
    "    def _init_processors(self):\n",
    "        task = self.config.task\n",
    "        task_config = getattr(self.config.dataset_config, task)\n",
    "        image_processor_config = task_config.processors.image_test_processor\n",
    "        answer_processor_config = task_config.processors.answer_processor\n",
    "        \n",
    "        self.dataset = build_dataset(self.config, \"test\", self.current_device)\n",
    "        self.image_processor = self.dataset.image_test_processor\n",
    "        self.answer_processor = self.dataset.answer_processor\n",
    "        self.prepare = ConvertCocoPolysToMask(task_config[\"use_mask\"])\n",
    "\n",
    "    def _build_boxer(self, model_path):\n",
    "        num_classes = self.dataset.get_answer_size()\n",
    "        model = build_model(self.config, num_classes)\n",
    "        \n",
    "        ext = model_path.split(\".\")[-1]\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "        if ext == \"ckpt\":\n",
    "            state_dict = state_dict[\"model\"]\n",
    "            \n",
    "        if list(state_dict.keys())[0].startswith('module') and not hasattr(model, 'module'):\n",
    "            state_dict = self._multi_gpu_state_to_single(state_dict)\n",
    "        \n",
    "        print(\"Loading model:\", model.load_state_dict(state_dict))\n",
    "        model.to(self.current_device)\n",
    "        model.inference()\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def _multi_gpu_state_to_single(self, state_dict):\n",
    "        new_sd = {}\n",
    "        for k, v in state_dict.items():\n",
    "            if not k.startswith('module.'):\n",
    "                raise TypeError(\"Not a multiple GPU state of dict\")\n",
    "            k1 = k[7:]\n",
    "            new_sd[k1] = v\n",
    "        return new_sd\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, image, image_id=0, threshold=0.5):\n",
    "        visualizer = Visualizer(np.asarray(image)[:,:,::-1])\n",
    "        \n",
    "        target = {\"image_id\": image_id, \"annotations\": []}\n",
    "        image, target = self.prepare(image, target)\n",
    "        sample = {\"image\": image}\n",
    "        \n",
    "        sample, target = self.image_processor(sample, target)\n",
    "        \n",
    "        h, w = sample[\"image\"].shape[-2:]\n",
    "        sample[\"image\"] = sample[\"image\"][None]\n",
    "        sample[\"mask\"] = sample[\"image\"].new_zeros(1, h, w).bool()\n",
    "        target = [target]\n",
    "        \n",
    "        sample = {\n",
    "            k: v.to(self.current_device) for k, v in sample.items()\n",
    "        }\n",
    "        target = [\n",
    "            {k: v.to(self.current_device) for k, v in t.items()}\n",
    "            for t in target\n",
    "        ]\n",
    "        \n",
    "        output = self.model(sample, target)\n",
    "        results = self.dataset.format_for_evalai(output, target, threshold=threshold, return_rles=True)\n",
    "        \n",
    "        if len(results[image_id][\"boxes\"]) > 0:\n",
    "            boxes = results[image_id][\"boxes\"].detach().cpu().numpy()\n",
    "            labels = [self.answer_processor.idx2cls(label) for label in results[image_id][\"labels\"].tolist()]\n",
    "            masks = results[image_id][\"rles\"]\n",
    "\n",
    "            visualizer.overlay_instances(boxes=boxes, labels=labels, masks=masks)\n",
    "        \n",
    "        return visualizer.get_output().get_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = BoxeRDemo(log_dir, model_path, config_path, torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://farm1.staticflickr.com/164/391500639_fb1c5de6df_z.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "image_vis = demo.predict(image, threshold=0.3)\n",
    "vis.imshow(image_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization test-dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annFile = os.path.join(os.environ[\"E2E_DATASETS\"], \"coco/annotation/image_info_test-dev2017.json\")\n",
    "image_path = os.path.join(os.environ[\"E2E_DATASETS\"], \"coco/image/test2017/{:012d}.jpg\")\n",
    "test_coco = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = CocoDetection(os.path.join(os.environ[\"E2E_DATASETS\"], \"coco/image/test2017\"), annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 247\n",
    "image, target = testset[idx]\n",
    "img_id = testset.ids[idx]\n",
    "\n",
    "img = cv2.imread(image_path.format(img_id))\n",
    "vis.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vis = demo.predict(image, img_id, 0.3)\n",
    "vis.imshow(image_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isabs(vis_path):\n",
    "    vis_path = os.path.join(log_dir, vis_path)\n",
    "\n",
    "if not os.path.exists(vis_path):\n",
    "    os.makedirs(vis_path, exist_ok=True)\n",
    "    \n",
    "for i in range(1000):\n",
    "    if i % 50 == 0 and i > 0:\n",
    "        print(f\"{i}/1000\")\n",
    "    image, target = testset[i]\n",
    "    img_id = testset.ids[i]\n",
    "    image_vis = demo.predict(image, img_id, threshold=0.3)\n",
    "    vis.imsave(os.path.join(vis_path, \"test_{}.png\".format(i)), image_vis)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
